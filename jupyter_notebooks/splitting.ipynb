{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e0b9b3-5564-4831-a10f-6ebf8d48273a",
   "metadata": {},
   "source": [
    "# Splitting LAZ files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e512b7a-49e5-48ee-b11b-7b399a1ef64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd75b141-aee8-4e9e-9965-5294e469065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "laz_files = [\"./AHN3-0.laz\",\"./AHN3-1.laz\",\"./AHN3-2.laz\"]  # list of files \n",
    "max_filesize = 0.2 * 2**20  # desired max file size (in bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6170c809-39c2-4d8c-9171-05413156fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAZ_COMPRESSION_FACTOR = 7\n",
    "\n",
    "def save_chunk_to_laz_file(in_filename, \n",
    "                           out_filename, \n",
    "                           offset, \n",
    "                           n_points):\n",
    "    \"\"\"Read points from a LAS/LAZ file and write them to a new file.\"\"\"\n",
    "    \n",
    "    points = np.array([])\n",
    "    \n",
    "    with laspy.open(in_filename) as in_file:\n",
    "        with laspy.open(out_filename, \n",
    "                        mode=\"w\", \n",
    "                        header=in_file.header) as out_file:\n",
    "            in_file.seek(offset)\n",
    "            points = in_file.read_points(n_points)\n",
    "            out_file.write_points(points)\n",
    "    return len(points)\n",
    "\n",
    "def split_strategy(filename, max_filesize):\n",
    "    \"\"\"Set up splitting strategy for a LAS/LAZ file.\"\"\"\n",
    "    with laspy.open(filename) as f:\n",
    "        bytes_per_point = (\n",
    "            f.header.point_format.num_standard_bytes +\n",
    "            f.header.point_format.num_extra_bytes\n",
    "        )\n",
    "        n_points = f.header.point_count\n",
    "    n_points_target = int(\n",
    "        max_filesize * LAZ_COMPRESSION_FACTOR / bytes_per_point\n",
    "    )\n",
    "    stem, ext = os.path.splitext(filename)\n",
    "    return [\n",
    "        (filename, f\"{stem}-{n}{ext}\", offset, n_points_target)\n",
    "        for n, offset in enumerate(range(0, n_points, n_points_target))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d3c418-7aaa-460d-a79f-f7194ee63e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up calculation\n",
    "files = db.from_sequence(laz_files) \n",
    "input_args = files.map(split_strategy, max_filesize=max_filesize) \\\n",
    "    .flatten() \\\n",
    "    .unzip(4)  # unpack input arguments\n",
    "res = db.map(save_chunk_to_laz_file, *input_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee00d15-4ba4-4ae6-ad45-2bc30c9d6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Dask cluster before this cell!\n",
    "tot_points = res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91b79d7-e492-4d8b-9707-07195f6623ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260807"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitted points\n",
    "sum(tot_points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
